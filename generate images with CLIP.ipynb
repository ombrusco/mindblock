{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/openai/CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import clip\n",
    "import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(device, shape, lr, prompt, clip_model):\n",
    "\n",
    "    image=torch.rand((1, 3, shape[0], shape[1]), device=device, requires_grad=True)\n",
    "\n",
    "    opt=torch.optim.Adam((image,),lr)\n",
    "\n",
    "    f=transforms.Compose([lambda x:torch.clamp((x+1)/2,min=0,max=1),transforms.RandomAffine(degrees=60, translate=(0.1, 0.1)),transforms.RandomGrayscale(p=0.2),\n",
    "                        transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.01),transforms.Resize(224)])\n",
    "\n",
    "    m=clip.load(clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
    "\n",
    "    embedding=m.encode_text(clip.tokenize(prompt).to(device))\n",
    "\n",
    "    return image, opt, f, m, embedding\n",
    "\n",
    "def total_variation_loss(img):\n",
    "    yv = torch.pow(img[:,:,1:,:]-img[:,:,:-1,:], 2).sum()\n",
    "    xv = torch.pow(img[:,:,:,1:]-img[:,:,:,:-1], 2).sum()\n",
    "    return (yv+xv)/(1*3*shape[0]*shape[1])\n",
    "\n",
    "def spherical_distance_loss(x, y):\n",
    "    return (torch.nn.functional.normalize(x, dim=-1) - torch.nn.functional.normalize(y, dim=-1)).norm(dim=-1).div(2).arcsin().pow(2).mul(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a prompt\n",
    "\n",
    "device ='cpu'\n",
    "cutn =  16\n",
    "shape = (256, 256) \n",
    "clip_model = \"ViT-B/32\"\n",
    "lr = 0.03\n",
    "prompt = \"Paste here your prompt to generate an image\"\n",
    "steps = 500\n",
    "\n",
    "image, opt, f, m, embedding = get_embedding(device, shape, lr, prompt, clip_model)\n",
    "for i in trange(steps):\n",
    "    opt.zero_grad()\n",
    "    clip_in = m.encode_image(torch.cat([f(image.add(1).div(2)) for _ in range(cutn)]))\n",
    "    loss = spherical_distance_loss(clip_in, embedding.unsqueeze(0)) + (image - image.clamp(-1, 1)).pow(2).mean()/2 + total_variation_loss(image)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "img = transforms.ToPILImage()(image.squeeze(0).clamp(-1,1)/2+.5)\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple images with different parameters and prompts, then save them.\n",
    "# Learning rate, number of steps and part of the prompt are saved in the name. \n",
    "\n",
    "Path(\"/gen_imgs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device ='cpu'\n",
    "cutn =  16\n",
    "shape = (256, 256) \n",
    "clip_model = \"ViT-B/32\"\n",
    "\n",
    "for document in ['patient utterances.txt', 'interpretation of G. Bateson.txt']:\n",
    "    with open(document, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        if lines != []:\n",
    "            for prompt in lines:\n",
    "                prompt = prompt[:-2]\n",
    "\n",
    "                for lr in [0.02, 0.03, 0.04]:\n",
    "                    for steps in [400, 500, 600]:\n",
    "\n",
    "                        image, opt, f, m, embedding = get_embedding(device, shape, lr, prompt, clip_model)\n",
    "                        for i in trange(steps):\n",
    "                            opt.zero_grad()\n",
    "                            clip_in = m.encode_image(torch.cat([f(image.add(1).div(2)) for _ in range(cutn)]))\n",
    "                            loss = spherical_distance_loss(clip_in, embedding.unsqueeze(0)) + (image - image.clamp(-1, 1)).pow(2).mean()/2 + total_variation_loss(image)\n",
    "                            loss.backward()\n",
    "                            opt.step()\n",
    "                        \n",
    "                        img = transforms.ToPILImage()(image.squeeze(0).clamp(-1,1)/2+.5)\n",
    "                        img.save(f'gen_imgs/{document[:10]} {prompt[:35]} lr:{lr} steps:{steps}.png')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9462c7ef87b517ebf5f2d4eec9ff1423ec6f711162eada97c43b4db424b3c875"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
